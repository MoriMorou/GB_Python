{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Lesson_8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIAqr2CPdfJe",
        "colab_type": "text"
      },
      "source": [
        "# Урок 8. Снижение размерности данных "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOcLKigsdfJg",
        "colab_type": "text"
      },
      "source": [
        "Большая размерность данных (под ней понимается размерность пространства признаков, то есть их количество) может серьезно усложнить задачу анализа таких данных и даже стать причиной некорректной работы некоторых алгоритмов. Кроме того, часто в исходных данных могут присутствовать лишние признаки, никак не связанные с целевой переменной. Поэтому часто встает задача понижения количества признаков, оставляя при этом самые значимые (наиболее сильно влияющие на значение целевого параметра) с отсечением менее значимых (наиболее слабо коррелирующих со значением целевого параметра) или с формированием новых признаков на основе старых. То есть ставится задача перехода от пространства большей размерности к пространству меньшей размерности с сохранением максимального количества полезной информации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pa5-7u-dfJh",
        "colab_type": "text"
      },
      "source": [
        "## Алгоритмы снижения размерности"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zspAp10fdfJi",
        "colab_type": "text"
      },
      "source": [
        "Алгоритмы снижения размерности пространства признаков делятся на две группы - отбор признаков (то есть отбрасывание наименее важных признаков) и понижение размерности путем формирования новых признаков на основе старых."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmvx3YA5dfJj",
        "colab_type": "text"
      },
      "source": [
        "### Отбор признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isLulqvJdfJk",
        "colab_type": "text"
      },
      "source": [
        "Самым простым и примитивным методом отбора является _одномерный отбор признаков_. Он заключается в оценке предсказательной силы каждого признака, то есть его информативности - насколько он коррелирует с целевой переменной. Затем отбираются либо заданное количество $k$ признаков, либо те признаки, информативность которых выше некоторого порога."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U547aIwTdfJl",
        "colab_type": "text"
      },
      "source": [
        "Оценка предсказательной силы признака (или степени связи этого признака и целевой переменной) может проводиться разными методами, например:\n",
        "\n",
        "- в случае регрессии - _корреляция_ $$R_{j} = \\frac{\\sum_{i=1}^{l}(x_{ij} - \\bar{x}_{j})(y_{i} - \\bar{y})}{\\sqrt{\\sum_{i=1}^{l}(x_{ij} - \\bar{x}_{j})^{2}\\sum_{i=1}^{l}(y_{i} - \\bar{y})^{2}}},$$ где $\\bar{x_{j}}$ и $\\bar{y}$ - среднее значение $j$-го признака и целевой переменной, соответственно. Чем больше по модулю корреляция ($\\pm 1$), тем информативнее признак. Следует заметить, что этот метод учитывает только линейную связь между признаком и целевой переменной.\n",
        "\n",
        "\n",
        "- в случае задачи классификации - _взаимная информация (mutual information)_, моделирующая корреляцию между признаками и классами. Она использует в расчете вероятность того, что одновременно значение $j$-го признака $x_{ij}$ равно числу $v$ и значение целевой переменной $y_{i}=k$, или, другими словами, долю таких объектов от общего количества объектов в выборке $P(x=v,y=k)$. Тогда взаимная информация будет находиться как $$MI_{j}=\\sum_{v \\in X}\\sum_{k \\in Y}P(x=v,y=k)\\text{log}\\frac{P(x=v,y=k)}{P(x=v)P(y=k)}.$$ Здесь $P(x=v)$ и $P(y=k)$ - доли объектов, на которых значение признака равно $v$ и значение целевой переменной равно $k$, соответственно. Если признак и целевая переменная независимы, то взаимная информация обращается в ноль. В отличие от предыдущего метода, этот метод позволяет находять произвольные зависимости (в т.ч. нелинейные) в пространстве произвольной размерности.\n",
        "\n",
        "Такие методы позволяют оценить важность исключительно каждого признака отдельно, без учета влияния комбинаций признаков на целевую переменную, поэтому они и называются одномерными. На практике зачастую признаки влияют именно в совокупности, и по отдельности могут ошибочно быть расценены как некоррелирующие с целевой переменной, поэтому одномерные методы отбора не являются оптимальным методом в большинстве случаев."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV5E5PnadfJl",
        "colab_type": "text"
      },
      "source": [
        "Отдельной группой методов можно назвать так называемые _переборные методы_, которые дискретно оценивают качество модели, обученной на различных подмножествах признаков. При этом происходит полный перебор всех возможных вариантов. Обычно такие алгоритмы делятся на _жадные (greedy)_ и _нежадные (non-greedy)_. Полный список их можно найти в дополнительных материалах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOjQFkTgdfJm",
        "colab_type": "text"
      },
      "source": [
        "Жадность алгоритмов заключаются в том, что если один из признаков включен в подмножество (или исключен в случае исключающего метода), в следующих итерациях поиска он уже не учитывается, так что алгоритм работает на меньшем объеме данных. Известные алгоритмы этого типа - _жадное включение_ и _жадное исключение_. В случае жадного включения на первой итерации аналогично одномерному отбору признаков находится признак, обладающий наибольшей предсказательной силой и добавляется в формирующуееся подмножество $\\{i_{1}\\}$. Далее происходит перебор оставшихся признаков с попеременным добавлением каждого из них в подмножество к первому и оценкой качества получаемой модели, обученной на подмножестве из этих двух признаков $\\{i_{1}, i_{2}\\}$. В итоге в подмножестве остается тот признак, при добавлении которого получается наилучшее качество. Далее эта процедура повторяется до момента, пока ошибка получаемой модели уменьшается. На каждой итерации в подмножество добавляется один признак, максимально улучшающий работу модели. Если на какой-то итерации при добавлении признаков ошибка не уменьшается, процесс останавливается.\n",
        "\n",
        "Плюсом такого алгоритма является относительная быстрота и возможность учета некоторых взаимодействий между признаками (как раз то, чего лишен одномерный отбор). Минусом же можно назвать вероятность застрять в локальном минимуме ошибки, если такой есть. В случае же когда есть единственный глобальный минимум, алгоритм найдет оптимальное решение.\n",
        "\n",
        "Есть также модификации этого алгоритма с многократным проходом по выборке и поочередным включением/исключением признаков из подмножества для учета совокупного влияния признаков."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5TOTIM4dfJo",
        "colab_type": "text"
      },
      "source": [
        "Примером нежадного алгоритма может быть простой последовательный полный перебор всех возможных подмножеств признаков. Такой подбор позволяет найти наиболее оптимальное подмножество признаков, но, очевидно, он является достаточно трудоемким (нужно перебрать $2^{n}$ вариантов, где $n$ - число признаков), поэтому подходит только для датасетов с небольшим количеством признаков."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqDrhuvidfJp",
        "colab_type": "text"
      },
      "source": [
        "Еще одна группа методов отбора признаков - _встроенные в модели_. Они используют эвристики, заложенные в обучающие модели, для оценки важности признаков.\n",
        "\n",
        "- Например, в случае работы с линейными моделями мы имеем зависимость целевой переменной от взвешенной суммы признаков $$a(x) = \\sum_{i=1}^{n}w_{i}x^{i}.$$ Здесь, если признаки масштабированы, веса будут являться показателями информативности признаков: чем больше вес, тем больший вклад данный признак вносит в значение целевой переменной. На основе этого показателя можно проводить отбор признаков. Также, вспоминая уроки по линейным моделям, можно упомянуть, что использование $L_{1}$-регуляризации приводит к занулению весов наименее важных признаков, то есть к их отбрасыванию, при этом больший коэффициент регуляризации будет приводить к большему количеству зануленных весов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMFGIpqQdfJr",
        "colab_type": "text"
      },
      "source": [
        "- В случае использования решающих деревьев и их композиций, где в каждой вершине происходит разбиение на два поддерева путем сравнивания значения одного признака с некоторым значением порога, важность признака можно оценивать по тому, насколько он уменьшает значение критерия информативности, по которому оценивается качество разбиения: $$Q(X_{m}, j, t) = H(X_{m}) - \\frac{|X_{l}|}{|X_{m}|}H(X_{l}) - \\frac{|X_{r}|}{|X_{m}|}H(X_{r}),$$ где $X_{m}$ - множество объектов, попавших в вершину на данном шаге, $X_{l}$ и $X_{r}$ - множества, попадающие в левое и правое поддерево, соответственно, после разбиения. $H(X)$ - критерий информативности. \n",
        "    \n",
        "    Чем сильнее падает критерий информативности при разбиении по данному признаку (то есть чем выше $Q$), тем этот признак важнее. Таким образом, важность $j$-го признака можно оценить путем вычисления суммы уменьшений критерия информативности по всем вершинам, в которых делалось разбиение по данному признаку. Чем больше эта сумма, тем важнее данный признак был при построении дерева. В случае композиций деревьев этот показатель суммируется по всем деревьям."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suyYET13dfJr",
        "colab_type": "text"
      },
      "source": [
        "### Понижение размерности"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w8aSllUdfJs",
        "colab_type": "text"
      },
      "source": [
        "Кроме отбора признаков, который не всегда оптимален в плане сохранения максимума полезной информации, существуют еще методы понижения размерности путем формирования новых признаков на основе старых. Новых признаков при использовании такого метода должно быть меньше, чем исходных, при условии сохранения максимально возможного количества информации из исходных признаков. Например, объединение нескольких признаков в линейную комбинацию:\n",
        "\n",
        "$$z_{ij}=\\sum_{k=1}^{n}w_{jk}x_{ik},$$\n",
        "\n",
        "где $x_{ij}$ - исходные признаки, $z_{ij}$ - новые принаки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2iJ7XBhdfJt",
        "colab_type": "text"
      },
      "source": [
        "Простейшим методов такого понижения размерности является метод _случайных проекций_, который заключается в преобразованиях, сохраняющих расстояния и снижающих размерности. Существование таких преобразований доказано для выборок, в которых объектов меньше, чем признаков. Веса при всех признаках в таком методе можно выбирать случайно. При этом не факт, что мы попадем в оптимальное преобразование, но практика показывает, что метод работает, если размерность нового пространства признаков\n",
        "\n",
        "$$d > \\frac{8\\text{ln}l}{\\varepsilon^{2}},$$\n",
        "\n",
        "где $l$ - количество объектов, $\\varepsilon$ - максимальное изменение расстояния между объектами (лемма о малом искажении или лемма Джонсона-Линденштрауса)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RKYQxgMdfJu",
        "colab_type": "text"
      },
      "source": [
        "#### Метод главных компонент (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7frA6OrdfJv",
        "colab_type": "text"
      },
      "source": [
        "Одним из наиболее известных и широко применяемых методов понижения размерности является _метод главных компонент (principal component analysis, PCA)_. Он заключается в приближении матрицы признаков матрицей меньшего ранга - так называемом низкоранговом приближении.\n",
        "\n",
        "Запишем показанную ранее формулу линейного преобразования признаков в матричном виде:\n",
        "\n",
        "$$Z = XW^{T},$$\n",
        "\n",
        "где $X$ - матрица \"объекты-признаки\", где по строкам отложены объекты, а по столбцам - значения признаков, $Z$ - матрица новых признаков, $W^{T}$ - транспонированная матрица весов. Приближение заключается формировании новой матрицы признаков $\\tilde{X}=ZW\\approx X$ с возможностью восстанавливания старых признаков по новым с максимальным уровнем точности, или, если говорить иначе, чтобы их различие было минимальным:\n",
        "\n",
        "$$\\|ZW - X\\|^{2} \\rightarrow \\underset{Z, W}{\\text{min}}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oGhpkBfdfJv",
        "colab_type": "text"
      },
      "source": [
        "При этом метод главных компонент предполагает, что матрица весов должна быть ортогональной, то есть произведение $WW^{T}$ должно равняться единичной матрице. Восстановленная матрица $ZW$ может иметь ранг меньший, чем исходная $X$, поэтому приближение будет называться низкоранговым."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P45TTDjedfJw",
        "colab_type": "text"
      },
      "source": [
        "Геометрически метод можно представить как проецирование признаков на гиперплоскость с максимизацией дисперсии получаемой выборки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dV8i8cDdfJx",
        "colab_type": "text"
      },
      "source": [
        "Если ранг матрицы исходных признаков $rank(X) \\geq d$, где $d$ - число новых признаков, то минимум функционала различия, описанного выше, достигается тогда, когда в качестве строк матрицы $W$ используются собственные векторы матрицы $X^{T}X$ , соответствующие максимальным собственным значениям $\\lambda_{1},...,\\lambda_{d}$. Максимальные собственные значения и называются главными компонентами, от чего пошло название метода. Первая главная компонента соответствует максимальному собственному значению и т.д."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvLN4XJLdfJx",
        "colab_type": "text"
      },
      "source": [
        "Некоторые полезные свойства метода:\n",
        "\n",
        "- Матрица $Z$ при этом будет такой, что $Z^{T}Z = \\Lambda = diag(\\lambda_{1},...,\\lambda_{d})$.\n",
        "\n",
        "\n",
        "- Минимизированный функционал ошибки будет равен $$\\|ZW - X\\|^{2} = \\|X\\|^{2} - tr\\Lambda,$$ где $tr\\Lambda,$ - след матрицы $\\Lambda$, то есть сумма всех собственных значений $\\lambda_{1},...,\\lambda_{d}$, а $\\|X\\|^{2}$ - сумма всех собственных значений исходной матрицы $\\lambda_{1},...,\\lambda_{n}$, таким образом $$\\|ZW - X\\|^{2} = \\sum_{j=d+1}^{n}\\lambda_{j},$$ то есть значение функционала ошибки будет равно сумме собственных значений, которые не были взяты в получаемое разложение. Поэтому логично брать в разложение максимальные собственные значения, оставляя минимальные.\n",
        "\n",
        "\n",
        "- Матрица $X^{T}X$ - матрица ковариации, то есть матрица, которая характеризует дисперсию выборки. Дисперсия выборки после проецирования будет равна собственному значению $\\lambda$, поэтому логично, что первым берется собственный вектор, соответствующий максимальному собственному значению - нам нужно сохранить максимум дисперсии."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pPzjbW8dfJy",
        "colab_type": "text"
      },
      "source": [
        "Таким образом, для реализации метода главных компонент нужно :\n",
        "- найти собственные значения матрицы $X^{T}X$;\n",
        "- отобрать $d$ максимальных;\n",
        "- составить матрицу $W^{T}$, столбцы которой будут являться собственными векторами, соответствующими отобранным собственным значениям, расположенным в порядке убывания;\n",
        "- получить новую матрицу \"объекты-признаки\", умножив исходную матрицу $X$ на матрицу весов $W$:\n",
        "\n",
        "$$Z=XW.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnYvDiOadfJz",
        "colab_type": "text"
      },
      "source": [
        "#### PCA и SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thkg-tSXdfJ0",
        "colab_type": "text"
      },
      "source": [
        "Сформулировав принцип реализации метода главных компонент, нельзя не заметить его родство с сингулярным разложением матриц (SVD). Вспомним, что сингулярное разложение матрицы - это разложение вида\n",
        "\n",
        "$$X=UDV^{T},$$\n",
        "\n",
        "где столбцы ортогональной матрицы $U$ - это собственные векторы матрицы $XX^{T}$, столбцы ортогональной матрицы $V$ - собственные векторы матрицы $X^{T}X$, а на главной диагонали диагональной матрицы $D$ расположены собственные значения матриц $XX^{T}$ и $X^{T}X$ (они равны и также называются сингулярными числами матрицы $X$).\n",
        "\n",
        "Если число новых признаков $d$ равно старому числу признаков $n$, то можно приравнять разложения\n",
        "\n",
        "$$X=ZW=UDV^{T}.$$\n",
        "\n",
        "При этом матрицы $W$ и $V^{T}$ состоят из собственных векторов матрицы $X^{T}X$, то есть они равны при $Z=UD$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tzfpn9zdfJ0",
        "colab_type": "text"
      },
      "source": [
        "Получается, что метод опорных векторов - в своем роде \"урезанная версия\" сингулярного разложения, из которого убрали минимальные собственные значения с соответствующими собственными векторами. \n",
        "Таким образом, для реализации понижения размерности методом опорных векторов с помощью SVD нужно:\n",
        "- найти сингулярное разложение вектора $X$;\n",
        "- сформировать из столбцов матрицы $V$, соответствующих $d$ наибольшим сингулярным числам, матрицу весов $W$;\n",
        "- получить новую матрицу \"объекты-признаки\", умножив исходную матрицу $X$ на матрицу весов $W$:\n",
        "\n",
        "$$Z=XW.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWEQalcAdfJ1",
        "colab_type": "text"
      },
      "source": [
        "Для закрепления теории реализуем PCA с помощью Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSmUQy4ZdfJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3d93Gx2dfJ6",
        "colab_type": "code",
        "outputId": "eaee1c83-f56e-474c-a5f2-bdd5d4659ab4",
        "colab": {}
      },
      "source": [
        "# Загрузим игрушечный датасет из sklearn\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE6X-RyIdfJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Для начала отмасштабируем выборку\n",
        "X_ = X.astype(float)\n",
        "\n",
        "rows, cols = X_.shape\n",
        "\n",
        "# центрирование - вычитание из каждого значения среднего по строке\n",
        "means = X_.mean(0)\n",
        "for i in range(rows):\n",
        "    for j in range(cols):\n",
        "        X_[i, j] -= means[j]\n",
        "\n",
        "# деление каждого значения на стандартное отклонение\n",
        "std = np.std(X_, axis=0)\n",
        "for i in range(cols):\n",
        "    for j in range(rows):\n",
        "        X_[j][i] /= std[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItFOmLW9dfKB",
        "colab_type": "code",
        "outputId": "0b507805-c106-4539-d15a-4dc8327b764f",
        "colab": {}
      },
      "source": [
        "# Найдем собственные векторы и собственные значения\n",
        " \n",
        "covariance_matrix = X_.T.dot(X_)\n",
        "\n",
        "eig_values, eig_vectors = np.linalg.eig(covariance_matrix)\n",
        "\n",
        "# сформируем список кортежей (собственное значение, собственный вектор)\n",
        "eig_pairs = [(np.abs(eig_values[i]), eig_vectors[:, i]) for i in range(len(eig_values))]\n",
        "\n",
        "# и отсортируем список по убыванию собственных значений\n",
        "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "print('Собственные значения в порядке убывания:')\n",
        "for i in eig_pairs:\n",
        "    print(i[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Собственные значения в порядке убывания:\n",
            "437.7746724797988\n",
            "137.10457072021055\n",
            "22.013531335697195\n",
            "3.107225464292886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mdEDwm2dfKD",
        "colab_type": "text"
      },
      "source": [
        "Оценим долю дисперсии, которая описывается найденными компонентами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct-1I70mdfKE",
        "colab_type": "code",
        "outputId": "ab04c5c2-649d-4f6f-e849-3791203481d2",
        "colab": {}
      },
      "source": [
        "eig_sum = sum(eig_values)\n",
        "var_exp = [(i / eig_sum) * 100 for i in sorted(eig_values, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "print(f'Доля дисперсии, описвыаемая каждой из компонент \\n{var_exp}')\n",
        "\n",
        "# а теперя оценим кумулятивную (то есть накапливаемую) дисперсию при учитывании каждой из компонент\n",
        "print(f'Кумулятивная доля дисперсии по компонентам \\n{cum_var_exp}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Доля дисперсии, описвыаемая каждой из компонент \n",
            "[72.96244541329987, 22.850761786701778, 3.6689218892828697, 0.5178709107154814]\n",
            "Кумулятивная доля дисперсии по компонентам \n",
            "[ 72.96244541  95.8132072   99.48212909 100.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE15sfLtdfKH",
        "colab_type": "text"
      },
      "source": [
        "Таким образом, первая главная компонента описывает почти 73% информации, а первые две в сумме - 95.8%. В то же время последняя компонента описывает всего 0.5% и может быть отброжена без страха значительных потерь в качестве нашего анализа. Мы отбросим последние две компоненты, оставив первые две."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4xcGvf7dfKI",
        "colab_type": "code",
        "outputId": "e406100f-3bd4-40fb-ce09-15db12e77a12",
        "colab": {}
      },
      "source": [
        "# Сформируем вектор весов из собственных векторов, соответствующих первым двум главным компонентам\n",
        "W = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))\n",
        "\n",
        "print(f'Матрица весов W:\\n', W)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Матрица весов W:\n",
            " [[ 0.52106591 -0.37741762]\n",
            " [-0.26934744 -0.92329566]\n",
            " [ 0.5804131  -0.02449161]\n",
            " [ 0.56485654 -0.06694199]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n04tyd6xdfKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Сформируем новую матрицу \"объекты-признаки\"\n",
        "Z = X_.dot(W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBqTYO6udfKO",
        "colab_type": "code",
        "outputId": "8e8e6a9d-f8fa-4c1a-ea3f-84a9bd859bed",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "y = iris.target\n",
        "for c, i in zip(\"rgb\", [0, 1, 2]):\n",
        "    plt.scatter(Z[y==i, 0], Z[y==i, 1], c=c)\n",
        "plt.xlabel('Главная компонента 1')\n",
        "plt.ylabel('Главная компонента 2')\n",
        "plt.title('PCA датасета IRIS')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7P4Be8PdfKQ",
        "colab_type": "text"
      },
      "source": [
        "Таким образом, мы перешли от четырехмерного пространства признаков к двумерному и при этом классы остались разделимы в пространстве, то есть классификация возможна."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q896KWx0dfKR",
        "colab_type": "text"
      },
      "source": [
        "PCA наиболее хорошо работает, когда собственные значения $\\lambda$ на каком-то участке графика распределения убывают скачкообразно (критерий крутого склона), другими словами, если существуют предпосылки к тому, что следует решать задачу в пространстве меньшей размерности. Если же они убывают монотонно, следует рассмотреть вариант использования других методов работы с пространством признаков."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6YHvcPvdfKS",
        "colab_type": "text"
      },
      "source": [
        "## Дополнительные материалы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQKbma5FdfKT",
        "colab_type": "text"
      },
      "source": [
        "1. [Методы отбора признаков](https://habr.com/ru/company/aligntechnology/blog/303750/)\n",
        "2. [Взаимная информация](https://ru.wikipedia.org/wiki/%D0%92%D0%B7%D0%B0%D0%B8%D0%BC%D0%BD%D0%B0%D1%8F_%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D1%8F)\n",
        "3. [Методы понижения размерности](http://www.machinelearning.ru/wiki/images/0/06/SLT%2C_lecture_8.pdf)\n",
        "4. [Лемма о малом искажении](https://ru.wikipedia.org/wiki/%D0%9B%D0%B5%D0%BC%D0%BC%D0%B0_%D0%BE_%D0%BC%D0%B0%D0%BB%D0%BE%D0%BC_%D0%B8%D1%81%D0%BA%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B8)\n",
        "5. [PCA from Scratch in Python](https://github.com/bhattbhavesh91/pca-from-scratch-iris-dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Oi5NQ2dfKU",
        "colab_type": "text"
      },
      "source": [
        "## Домашнее задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-eNSDPndfKU",
        "colab_type": "text"
      },
      "source": [
        "1. (*) Написать свою реализацию метода главных компонент с помощью сингулярного разложения с использованием функции [numpy.linalg.svd()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html)\n",
        "2. (*) Обучить любую модель классификации на датасете IRIS до применения PCA и после него. Сравнить качество классификации по отложенной выборке."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUx79lzGxT91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}