{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGvPz-PJYo5C"
   },
   "source": [
    "# Урок 8\n",
    "\n",
    "## Дисперсионный анализ. Факторный анализ. Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "z4odg-NJYo5I"
   },
   "source": [
    "### Дисперсионный анализ\n",
    "\n",
    "__Дисперсионный анализ__ — метод в математической статистике, направленный на поиск зависимостей в экспериментальных данных путём исследования значимости различий в средних значениях. В дисперсионном анализе изучается влияние одного или нескольких факторов на зависимую переменную, причём факторы являются _номинативными_ (_категориальными_), а целевая переменная является _абсолютной_ (_количественной_).\n",
    "\n",
    "В __однофакторном дисперсионном анализе__ на одну переменную $Y$ влияет один фактор, наблюдаемый на $k$ _уровнях_, т.е. имеем $k$ выборок для переменной $Y$. Проверяется гипотеза $H_0$ о равенстве средних значений по каждой выборке:\n",
    "\n",
    "$$H_0: \\: \\overline{y_1} = \\dots = \\overline{y_k}.$$\n",
    "\n",
    "Наблюдаемые данные обозначим $y_{ij}$, где $i$ — индекс уровня ($i = 1, 2, \\dots, k$), $j$ — индекс наблюдения на $i$-м уровне ($j = 1, 2, \\dots, n_i$). Здесь $n_i$ - число наблюдений на $i$-м уровне. Таким образом, \n",
    "\n",
    "$$y_i = \\{ y_{i1}, \\dots, y_{i n_i} \\}$$\n",
    "\n",
    "для $i = 1, 2, \\dots, k$. Соответственно, $\\overline{y_i}$ - среднее значение на выборке $y_i$. Обозначим через $n$ общее число наблюдений:\n",
    "\n",
    "$$n = \\displaystyle\\sum_{j=1}^{k} n_i.$$\n",
    "\n",
    "Среднее от всей выборки:\n",
    "\n",
    "$$\\overline{y} = \\dfrac{1}{n} \\displaystyle\\sum_{i=1}^k \\displaystyle\\sum_{j=1}^{n_i} y_{ij}.$$\n",
    "\n",
    "Сумма квадратов отклонений наблюдений от общего среднего:\n",
    "\n",
    "$$S^2 = \\displaystyle\\sum_{i=1}^{k} \\displaystyle\\sum_{j=1}^{n_i} ({y}_{ij} - \\overline{y})^2.$$\n",
    "\n",
    "Эту сумму можно разбить на сумму квадратов отклонений средних групповых значений от общего среднего значения $\\overline{y}$:\n",
    "\n",
    "$$S_F^2 = \\displaystyle\\sum_{i=1}^k (\\overline{y}_i - \\overline{y})^2 n_i$$\n",
    "\n",
    "и остаточную сумму квадратов отклонений:\n",
    "\n",
    "$$S_{res}^{2} = \\displaystyle\\sum_{i=1}^k \\displaystyle\\sum_{j=1}^{n_i} (y_{ij} - \\overline{y}_i)^2.$$\n",
    "\n",
    "Для этих значений должно быть справедливо равенство\n",
    "\n",
    "$$S^2 = S_F^2 + S_{res}^2.$$\n",
    "\n",
    "По этим значениям можно вычислить соответствующие несмещённые оценки дисперсий:\n",
    "\n",
    "$$\\sigma^2 = \\dfrac{S^2}{n - 1}, \\: \\sigma_F^2 = \\dfrac{S_F^2}{k - 1}, \\: \\sigma_{res}^2 = \\dfrac{S_{res}^2}{n - k}.$$\n",
    "\n",
    "Для проверки гипотезы $H_0$ запишем статистику\n",
    "\n",
    "$$T = \\dfrac{\\sigma_F^2}{\\sigma_{res}^2}.$$\n",
    "\n",
    "В предположении верности гипотезы $H_0$ статистика $T$ имеет распределение Фишера с параметрами $k_1 = k - 1$, $k_2 = n - k$. Для выбранного уровня значимости $\\alpha$ можно рассчитать критическое значение $F_{crit}$, равное квантилю порядка $1 - \\alpha$ для распределения $F(k_1, k_2)$. Если $T > F_{crit}$, то гипотеза $H_0$ отвергается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Пример 1__\n",
    "\n",
    "Среди людей, проживающих в одном городе, выделены три группы по профессии: бухгалтеры, юристы, программисты. Имеются данные по зарплатам в этих трёх группах:\n",
    "```\n",
    "y1 = [70, 50, 65, 60, 75]\n",
    "y2 = [80, 75, 90, 70, 75, 65, 85, 100]\n",
    "y3 = [130, 100, 140, 150, 160, 170, 200]\n",
    "```\n",
    "Требуется проверить гипотезу у равенстве средней зарплаты по каждой группе. Для проверки этой гипотезы воспользуемся однофакторным дисперсионным анализом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.array([70, 50, 65, 60, 75], dtype=np.float64)\n",
    "y2 = np.array([80, 75, 90, 70, 75, 65, 85, 100], dtype=np.float64)\n",
    "y3 = np.array([130, 100, 140, 150, 160, 170, 200], dtype=np.float64)\n",
    "\n",
    "n1 = y1.shape[0]\n",
    "n2 = y2.shape[0]\n",
    "n3 = y3.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выборочные средние по каждой группе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64.0, 80.0, 150.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_mean = y1.mean()\n",
    "y2_mean = y2.mean()\n",
    "y3_mean = y3.mean()\n",
    "\n",
    "y1_mean, y2_mean, y3_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что средние зарплаты разнятся. Проверим статистическую значимость этого отличия. \n",
    "\n",
    "Соберём все значения заработных плат в один массив:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 70.  50.  65.  60.  75.  80.  75.  90.  70.  75.  65.  85. 100. 130.\n",
      " 100. 140. 150. 160. 170. 200.]\n"
     ]
    }
   ],
   "source": [
    "y = np.concatenate([y1, y2, y3])\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_mean = y.mean()\n",
    "y_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём значения $S_F$ и $S_{res}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27175.0, 7270.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2_F = n1 * (y1_mean - y_mean) ** 2 + n2 * (y2_mean - y_mean) ** 2 + n3 * (y3_mean - y_mean) ** 2\n",
    "\n",
    "S2_res = ((y1 - y1_mean) ** 2).sum() + ((y2 - y2_mean) ** 2).sum() + ((y3 - y3_mean) ** 2).sum()\n",
    "\n",
    "S2_F, S2_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим выполнение равенства $S^2 = S_F^2 + S_{res}^2:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2_F + S2_res == ((y - y_mean) ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем оценки дисперсий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13587.5, 427.6470588235294)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "n = n1 + n2 + n3\n",
    "\n",
    "k1 = k - 1\n",
    "k2 = n - k\n",
    "\n",
    "sigma2_F = S2_F / k1\n",
    "sigma2_res = S2_res / k2\n",
    "\n",
    "sigma2_F, sigma2_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, значение статистики $T$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.77269601100413"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = sigma2_F / sigma2_res\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зафиксируем уровень значимости $\\alpha = 0.05$. Для него найдём критическое значение $F_{crit}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5915305684750827"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "F_crit = stats.f.ppf(1 - alpha, k1, k2)\n",
    "F_crit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что $T > F_{crit}$, поэтому заключаем, что отличие средних зарплат действительно является статистически значимым."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В __двухфакторном дисперсионном анализе__ на одну переменную $Y$ влияют два фактора $A$, $B$, каждый из которых является категориальным. Проверяются гипотезы о влиянии каждого фактора на значение $Y$. Отличие теперь в том, что влияния факторов на значение $Y$ могут \"пересекаться\", и это нужно учитывать.\n",
    "\n",
    "Рассмотрим схему __двухфакторного дисперсионного анализа с однократными наблюдениями__. При таком подходе для каждой пары уровней факторов $A$ и $B$ выполняется только одно измерение переменной $Y$.\n",
    "\n",
    "Пусть фактор $A$ имеет $m$ уровней, а фактор $B$ имеет $k$ уровней. Тогда исходные данные можно представить в виде таблицы\n",
    "\n",
    "$$y = \\left(\n",
    "\\begin{array}{ccc}\n",
    "y_{11} & \\dots & y_{1k} \\\\\n",
    "y_{21} & \\dots & y_{2k} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "y_{m1} & \\dots & y_{mk}\n",
    "\\end{array}\n",
    "\\right),$$\n",
    "\n",
    "где $y_{ij}$ - наблюдение на $i$-м уровне фактора $A$ и $j$-м уровне фактора $B$.\n",
    "\n",
    "_Замечание_. В двухфакторном дисперсионном анализе с многократными наблюдениями каждый $y_{ij}$ представлял бы собой какой-то массив из значений.\n",
    "\n",
    "По каждому фактору проверяется нулевая гипотеза о равенстве средних значений на каждом уровне. Пусть $y_{i \\ast}$ обозначает $i$-ю строку, а $y_{\\ast j}$ обозначает $j$-й столбец. В таких обозначениях $y_{i \\ast}$ соответствует значениям переменной $Y$ на $i$-м уровне фактора $A$ и $k$ уровнях фактора $B$. Наоборот, $y_{\\ast j}$ - значения переменной $Y$ на $m$ уровнях фактора $A$ и $j$-м уровне фактора $B$.\n",
    "\n",
    "Нулевые гипотезы можно записать в следующем виде:\n",
    "\n",
    "$$H_{0A}: \\: \\overline{y_{1 \\ast}} = \\dots = \\overline{y_{m \\ast}}, \\:\n",
    "H_{0B}: \\: \\overline{y_{\\ast 1}} = \\dots = \\overline{y_{\\ast k}}.$$\n",
    "\n",
    "Далее аналогичным образом вычисляются оценки дисперсий. Сначала считаем суммы квадратов отклонений:\n",
    "\n",
    "$$S_A^2 = k \\cdot \\displaystyle\\sum_{i=1}^m \\left( \\overline{y_{i \\ast}} - \\overline{y} \\right)^2, \\:\n",
    "S_B^2 = m \\cdot \\displaystyle\\sum_{j=1}^k \\left( \\overline{y_{\\ast j}} - \\overline{y} \\right)^2,$$\n",
    "$$S_{res}^2 = \\displaystyle\\sum_{i=1}^m \\displaystyle\\sum_{j=1}^k \\left( y_{ij} - \\overline{y_{i \\ast}} - \\overline{y_{\\ast j}} + \\overline{y} \\right)^2.$$\n",
    "\n",
    "Оценки дисперсий:\n",
    "\n",
    "$$\\sigma_A^2 = \\dfrac{S_A^2}{m - 1}, \\: \\sigma_B^2 = \\dfrac{S_B^2}{k - 1}, \\:\n",
    "\\sigma_{res}^2 = \\dfrac{S_{res}^2}{(k - 1) (m - 1)}.$$\n",
    "\n",
    "По этим оценкам дисперсий вычисляются статистики:\n",
    "\n",
    "$$T_A = \\dfrac{\\sigma_A^2}{\\sigma_{res}^2}, \\: T_B = \\dfrac{\\sigma_B^2}{\\sigma_{res}^2}.$$\n",
    "\n",
    "Для выбранного уровня значимости $\\alpha$ по каждому фактору вычисляется соответствующее критическое значение. Для фактора $A$ критическое значение $F_{crit, A}$ - квантиль порядка $1 - \\alpha$ для распределения $F(k_{1A}, k_{2A})$, где $k_{1A} = m - 1$, $k_{2A} = n - m$. Здесь $n = k \\cdot m$ - общее число наблюдений. Аналогично, $F_{crit, B}$ - квантиль порядка $1 - \\alpha$ для распределения $F(k_{1B}, k_{2B})$, где $k_{1B} = k - 1$, $k_{2B} = n - k$.\n",
    "\n",
    "Наконец, проверяются гипотезы. Если $T_A > F_{crit, A}$, то отвергается гипотеза $H_{0A}$, если $T_B > F_{crit, B}$, отвергается гипотеза $H_{0B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Факторный анализ\n",
    "\n",
    "__Факторный анализ__ — это способ приведения множества непосредственно наблюдаемых факторов $x_j$, $j = 1, \\dots, m$, к меньшему числу новых линейно независимых факторов $y_j$, $j = 1, \\dots, q$, $q < m$.\n",
    "\n",
    "Рассмотрим __метод главных компонент__. Этот метод заключается в вычислении собственных значений и собственных векторов для ковариационной матрицы:\n",
    "\n",
    "$$\\operatorname{cov} = \\left(\n",
    "\\begin{array}{ccccc}\n",
    "\\sigma_1^2 & \\sigma_{12} & \\sigma_{13} & \\dots & \\sigma_{1m} \\\\\n",
    "\\sigma_{12} & \\sigma_2^2 & \\sigma_{23} & \\dots & \\sigma_{2m} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_{1m} & \\sigma_{2m} & \\sigma_{3m} & \\dots & \\sigma_m^2\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Посчитаем для этой матрицы собственные значения и расположим их в порядке убывания:\n",
    "\n",
    "$$\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_m.$$\n",
    "\n",
    "Метод главных компонент заключается в том, что если мы хотим получить $q$ признаков вместо $m$, наиболее выгодно взять $q$ наибольших собственных значений матрицы $\\operatorname{cov}$, а затем с помощью соответствующих собственных векторов получить новые $q$ признаков.\n",
    "\n",
    "Допустим, имеется матрица объект-признак:\n",
    "\n",
    "$$X = \\left(\n",
    "\\begin{array}{ccc}\n",
    "x_{11} & \\dots & x_{1m} \\\\\n",
    "x_{21} & \\dots & x_{2m} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n1} & \\dots & x_{nm}\n",
    "\\end{array}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "В $j$-м столбце $i$-й строки этой матрицы содержится значение признака $x_j$ для $i$-го наблюдения. \n",
    "\n",
    "Перед тем, как применять метод главных компонент, данную матрицу нужно __центрировать__, т.е. из каждого столбца вычесть среднее значение соответствующего признака. В результате получим матрицу $X^\\ast = \\left( x_{ij}^\\ast \\right)_{n \\times m}$, такую, что среднее значение по каждому столбцу матрицы $X^\\ast$ равно 0.\n",
    "\n",
    "Далее вычисляется матрица несмещённых выборочных ковариаций $\\operatorname{cov} = \\left( \\sigma_{ij} \\right)_{m \\times m}$, где $\\sigma_{ii} = \\sigma_i^2$ - дисперсия $i$-го признака.\n",
    "\n",
    "Пусть $v_1, \\dots, v_m$ - собственные векторы (столбцы) матрицы $\\operatorname{cov}$, соответствующие собственным значениям $\\lambda_1, \\dots, \\lambda_m$. \n",
    "\n",
    "Возьмём первые $q$ векторов: $v_1, \\dots, v_q$ и составим из этих столбцов матрицу $T$. Умножив матрицу $X^\\ast$ на матрицу $T$, мы получим матрицу $Y = X^\\ast \\cdot T$ размера $n \\times q$. Эта матрица состоит из значений новых признаков $y_i$.\n",
    "\n",
    "Метод главных компонент можно также применять без конкретного значения $q$, а просто с целью получить меньшее число признаков, при этом потеряв минимальное количество информации. Количество потерянной информации можно оценить с помощью __доли объяснённой дисперсии__. Сумма дисперсий признаков (расположенных на главной диагонали) называется __общей дисперсией__:\n",
    "\n",
    "$$\\sigma^2 = \\displaystyle\\sum_{i=1}^m \\sigma_i^2.$$\n",
    "\n",
    "Соответственно, долей объяснённой дисперсии $i$-го признака называется частное $\\dfrac{\\sigma_i^2}{\\sigma^2}$.\n",
    "\n",
    "Идея состоит в том, чтобы взять первые $q$ признаков $y_1, \\dots, y_q$ так, чтобы сумма их долей объяснённых дисперсий составляла не менее $90 \\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Пример 2__\n",
    "\n",
    "Рассмотрим матрицу объект признак:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.array([[5, 1.6],\n",
    "                    [4, 1.4],\n",
    "                    [6, 1.9],\n",
    "                    [3, 1.1],\n",
    "                    [4, 1.25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку признаков всего 2, соответствующие объекты можно расположить на плоскости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fd594de4c88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(samples[:, 0], samples[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, объекты практически лежат на одной прямой. Это означает, что мы можем попытаться заменить два признака на один, при этом не потеряв слишком много информации. Для этого воспользуемся методом главных компонент. Сначала центрируем наши данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6   0.15]\n",
      " [-0.4  -0.05]\n",
      " [ 1.6   0.45]\n",
      " [-1.4  -0.35]\n",
      " [-0.4  -0.2 ]]\n"
     ]
    }
   ],
   "source": [
    "samples_centered = samples - samples.mean(axis=0)\n",
    "\n",
    "print(samples_centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посчитаем матрицу ковариаций. Отметим, что если функция `numpy.cov` получает на вход двумерный массив, а не несколько одномерных массивов, то ожидается, что значения каждого признака будут расположены в строках (а не в столбцах, как у нас). Поэтому в эту функцию мы подаём транспонированную матрицу `samples_centered.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3    0.35  ]\n",
      " [0.35   0.0975]]\n"
     ]
    }
   ],
   "source": [
    "cov = np.cov(samples_centered.T)\n",
    "\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственные значения и собственные векторы можно теперь получить с помощью функции `numpy.linalg.eig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues:\n",
      "[1.39445221 0.00304779]\n",
      "\n",
      "eigenvectors:\n",
      "[[ 0.96546225 -0.26054298]\n",
      " [ 0.26054298  0.96546225]]\n"
     ]
    }
   ],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "print('eigenvalues:\\n{}\\n'.format(eigenvalues))\n",
    "print('eigenvectors:\\n{}'.format(eigenvectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим, что в массиве `eigenvectors` векторы записаны именно в столбцах.\n",
    "\n",
    "Итак, мы хотим получить один признак, поэтому возьмём первый вектор (он соответствует первому собственному значению, которое сильно больше второго). Домножим матрицу `samples_centered` на этот вектор, чтобы получить новую матрицу объект-признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6183588 ]\n",
      " [-0.39921205]\n",
      " [ 1.66198394]\n",
      " [-1.44283719]\n",
      " [-0.4382935 ]]\n"
     ]
    }
   ],
   "source": [
    "new_samples = samples_centered.dot(eigenvectors[:, [0]])\n",
    "\n",
    "print(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем долю объяснённой дисперсии этого нового признака. Общую дисперсию можно найти из матрицы ковариаций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3975"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_variance = cov[0, 0] + cov[1, 1]\n",
    "total_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем несмещённую оценку дисперсии нового признака и найдём его долю от общей дисперсии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9978191114161408"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance = new_samples.var(ddof=1)\n",
    "\n",
    "variance / total_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такое значение доли объяснённой дисперсии можно интерпретировать так: используя новые данные вместо старых, мы потеряем лишь $0.2 \\%$ информации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия\n",
    "\n",
    "Ранее мы познакомились с моделью линейной регрессии:\n",
    "\n",
    "$$y = b_0 + b_1 x_1 + \\dots + b_k x_k + \\varepsilon.$$\n",
    "\n",
    "Такая модель используется в задачах регрессии, т.е. когда нам нужно в результате получить какое-то число.\n",
    "\n",
    "__Логистическая регрессия__ применяется в задачах _бинарной классификации_, когда нам нужно получить на выходе метку класса: $1$ или $-1$ (иногда вместо $-1$ используют $0$). Логистическая регрессия представляет собой модель линейной регрессии, _поверх_ которой используется __логистическая функция__ (или __сигмоида__):\n",
    "\n",
    "$$\\sigma(x) = \\dfrac{1}{1 + e^{-x}}.$$\n",
    "\n",
    "Сигмоида принимает в качестве аргумента вещественное число, а отдаёт число из промежутка $[0, 1]$.\n",
    "\n",
    "Итак, модель логистической регрессии можно записать в виде:\n",
    "\n",
    "$$y = \\sigma(b_0 + b_1 x_1 + \\dots + b_k x_k) = \\sigma(x \\cdot b),$$\n",
    "\n",
    "где \n",
    "$b = \\left(\n",
    "\\begin{array}{c}\n",
    "b_0 \\\\\n",
    "\\vdots \\\\\n",
    "b_k\n",
    "\\end{array}\n",
    "\\right)$ - столбец коэффициентов регрессии,\n",
    "$x = \\left( x_0, \\dots, x_k \\right)$ - строка факторов. (Напомним, что $x_0 = 1$ - \"фиктивный\" фактор.)\n",
    "\n",
    "Такая модель на вход получает значения факторов, а на выходе отдаёт число из промежутка $[0, 1]$, которое можно интерпретировать как вероятность объекта принадлежать классу $1$.\n",
    "\n",
    "В отличие от линейной регрессии, для модели логистической регрессии уже не выходит записать аналитическое решение. Поэтому для нахождения оптимального решения используют __градиентный спуск__. При этом минимизируют функционал:\n",
    "\n",
    "$$Q(b) = \\displaystyle\\sum_{i=1}^m \\ln \\left( 1 + e^{- (x_i \\cdot b) y_i} \\right) \\rightarrow \\min_b.$$\n",
    "\n",
    "Здесь $x_i$ - $i$-й объект из выборки, $y_i$ - его метка класса ($1$ или $-1$).\n",
    "\n",
    "Зафиксируем следующие обозначения, чтобы не путаться:\n",
    "\n",
    "$$X = \\left(\n",
    "\\begin{array}{ccc}\n",
    "x_{10} & \\dots & x_{1k} \\\\\n",
    "x_{20} & \\dots & x_{2k} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_{m0} & \\dots & x_{mk}\n",
    "\\end{array}\n",
    "\\right), \\: \n",
    "y = \\left(\n",
    "\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_m\n",
    "\\end{array}\n",
    "\\right).$$\n",
    "\n",
    "Тогда функционал $Q$ можно переписать в виде:\n",
    "\n",
    "$$Q(b) = \\displaystyle\\sum_{i=1}^m \\ln \\left( 1 + e^{- (b_0 x_{i0} + \\dots + b_k x_{ik}) y_i} \\right).$$\n",
    "\n",
    "Чтобы использовать метод градиентного спуска, нам понадобится вектор градиента, который состоит из частных производных функционала $Q$ по переменным $b_j$:\n",
    "\n",
    "$$\\nabla Q = \\left(\n",
    "\\begin{array}{c}\n",
    "\\dfrac{\\partial Q}{\\partial b_0} \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial Q}{\\partial b_k}\n",
    "\\end{array}\n",
    "\\right),$$\n",
    "\n",
    "где $\\dfrac{\\partial Q}{\\partial b_j} = - \\displaystyle\\sum_{i=1}^m x_{ij} y_i \\left( 1 - \\sigma((b_0 x_{i0} + \\dots + b_k x_{ik}) y_i) \\right)$.\n",
    "\n",
    "Непосредственно метод градиентного спуска заключается в следующем. Сначала выбираются начальные значения параметров $b_0, \\dots, b_k$, т.е. вектор $b^{[0]}$. Затем итеративно повторяется вычисление:\n",
    "\n",
    "$$b^{[n + 1]} = b^{[n]} - \\lambda_n \\nabla Q(b^{[n]}).$$\n",
    "\n",
    "Параметр $\\lambda_n$ отвечает за скорость спуска. Он может быть\n",
    "\n",
    "* постоянным для всех $n$\n",
    "* убывающим по мере возрастания $n$\n",
    "* иным\n",
    "\n",
    "Описанный выше процесс повторяется, пока соседние векторы $b^{[n + 1]}$, $b^{[n]}$ не перестанут сильно отличаться друг от друга.\n",
    "\n",
    "Ниже приведена программная реализация градиентного спуска для функционала $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Сигмоида.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_derivative(j, b, x, y):\n",
    "    \"\"\"Частная производная функционала Q по переменной b_j.\n",
    "    \"\"\"\n",
    "    \n",
    "    return -sum(\n",
    "        x[i, j] * y[i] * (1 - sigmoid(b.dot(x[i]) * y[i]))\n",
    "        for i in range(x.shape[0])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(b, x, y):\n",
    "    \"\"\"Вектор градиента.\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array([\n",
    "        partial_derivative(j, b, x, y)\n",
    "        for j in range(b.shape[0])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(lambda_, b, x, y):\n",
    "    \"\"\"Один шаг градиентного спуска.\n",
    "    \"\"\"\n",
    "    \n",
    "    return b - lambda_ * gradient(b, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Пример 3__\n",
    "\n",
    "Рассмотрим массив `samples` из примера 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.   1.6 ]\n",
      " [4.   1.4 ]\n",
      " [6.   1.9 ]\n",
      " [3.   1.1 ]\n",
      " [4.   1.25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fd594d05048>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(samples)\n",
    "\n",
    "plt.scatter(samples[:, 0], samples[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пометим точки из правой верхней четверти меткой класса $1$, а остальные - меткой $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, -1, 1, -1, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим для данной задачи модель логистической регрессии методом градиентного спуска.\n",
    "\n",
    "Добавим столбец \"фиктивного\" фактора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   5.   1.6 ]\n",
      " [1.   4.   1.4 ]\n",
      " [1.   6.   1.9 ]\n",
      " [1.   3.   1.1 ]\n",
      " [1.   4.   1.25]]\n"
     ]
    }
   ],
   "source": [
    "ones = np.ones((samples.shape[0], 1))\n",
    "\n",
    "x = np.hstack([ones, samples])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам понадобится метрика чтобы вычислять расстояние между соседними векторами параметров. Будем использовать евклидово расстояние."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начальное приближение $b^{[0]}$ можно выбирать нулевым или произвольным. Будем также использовать коэффициент $\\lambda_n = \\dfrac{0.1}{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success on step 476\n",
      "\n",
      "[[-0.31287309]\n",
      " [ 0.08090979]\n",
      " [-0.05599962]]\n"
     ]
    }
   ],
   "source": [
    "b_0 = np.zeros(x.shape[1])\n",
    "# b_0 = np.random.uniform(-100, 100, size=x.shape[1])\n",
    "\n",
    "b = b_0\n",
    "\n",
    "for i in range(1, 10 ** 5):\n",
    "    lambda_ = 0.1 / i\n",
    "    \n",
    "    b_new = gradient_descent_step(lambda_, b, x, y)\n",
    "        \n",
    "    if euclidean(b, b_new) < 0.0001:\n",
    "        print(f'success on step {i}\\n')\n",
    "        \n",
    "        break\n",
    "    \n",
    "    b = b_new\n",
    "\n",
    "b = b_new.reshape(-1, 1)\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подобранных параметров получаем вектор из предсказанных значений $z = \\sigma(X \\cdot b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50051912]\n",
      " [0.48309809]\n",
      " [0.51654056]\n",
      " [0.46711173]\n",
      " [0.48519596]]\n"
     ]
    }
   ],
   "source": [
    "z = x.dot(b)\n",
    "\n",
    "z = np.array([sigmoid(z_i) for z_i in z])\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном векторе содержатся вероятности каждого объекта принадлежать классу $1$. Будем причислять объект к классу $1$ если даная вероятность больше 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_real: [ 1 -1  1 -1 -1]\n",
      "y_pred: [ 1 -1  1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array([\n",
    "    1 if z_ > 0.5 else -1\n",
    "    for z_ in z\n",
    "])\n",
    "\n",
    "print(f'y_real: {y}')\n",
    "print(f'y_pred: {y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что при некоторых значениях начального приближения $b^{[0]}$ метод градиентного спуска расходится. Это связано с тем, что начальное приближение оказывается слишком далеко от оптимальных значений, а $\\lambda_n$ быстро убывает с ростом $n$, поэтому шаги градиентного спуска становятся всё менее значительными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача 1__\n",
    "\n",
    "Провести дисперсионный анализ для определения того, есть ли различия среднего роста среди взрослых футболистов, хоккеистов и штангистов. Даны значения роста в трех группах случайно выбранных спортсменов: \n",
    "```\n",
    "football_players = [173, 175, 180, 178, 177, 185, 183, 182]\n",
    "hockey_players = [177, 179, 180, 188, 177, 172, 171, 184, 180]\n",
    "lifters = [172, 173, 169, 177, 166, 180, 178, 177, 172, 166, 170]\n",
    "```\n",
    "\n",
    "__Задача 2__\n",
    "\n",
    "Прикрепить ссылку на курсовой проект на github (см. выбор темы в ДЗ к Уроку 6).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Матстат - Урок 8.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
